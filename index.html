<!DOCTYPE HTML>
<html lang="en">

<!-- <head> -->
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Lei Yao | PhD Student</title>
  <meta name="author" content="Lei Yao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="static/images/PolyU.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">

</head>

<!-- body -->
<body>
  <div class="container">
    <header class="profile">
      <div class="profile-info">
        <h1 class="profile-name">Ray Lei Yao</h1>
        <p class="profile-intro">
          I am a PhD candidate with the
          <a href="https://www.polyu.edu.hk/eee/" target="_blank" rel="noopener noreferrer">Department of Electrical and Electronic Engineering</a>
          at the <a href="https://www.polyu.edu.hk/en/" target="_blank" rel="noopener noreferrer">Hong Kong Polytechnic University,</a>
          supervised by <a href="https://www.eie.polyu.edu.hk/~lpchau/" target="_blank" rel="noopener noreferrer"> Prof. Lap-Pui Chau</a>.
          I also work closely with <a href="https://wangyintu.github.io/" target="_blank" rel="noopener noreferrer">Dr. Yi Wang</a> @ PolyU and <a href="https://lmomoy.github.io/" target="_blank" rel="noopener noreferrer">Dr. Moyun Liu</a> @ HUST.
        </p>
        <p class="profile-intro">
          I am currently a visiting student in the <a href="https://www.cis.upenn.edu/" target="_blank" rel="noopener noreferrer">Department of Computer and Information Science </a> at <a href="https://www.upenn.edu/" target="_blank" rel="noopener noreferrer">University of Pennsylvania</a>, advised by <a href="https://lingjie0206.github.io/" target="_blank" rel="noopener noreferrer"> Prof. Lingjie Liu</a> within the <a href="https://www.grasp.upenn.edu/" target="_blank" rel="noopener noreferrer"> GRASP Lab</a>.
        </p>
        <p class="profile-intro">
          I recieved my BSc and MRes in the School of Mechanical Science and Engineering, <a href="http://english.hust.edu.cn/" target="_blank" rel="noopener noreferrer">Huazhong University of Science and Technology</a>
          advised by Prof. <a href="http://english.mse.hust.edu.cn/info/1046/1148.htm" target="_blank" rel="noopener noreferrer">Youping Chen</a> in 2020 and 2023, respectively.
        </p>
        <p class="profile-intro">
          I served as a research intern at <a href="https://www.tencent.com/en-us/business/robotics.html" target="_blank" rel="noopener noreferrer">Tencent Robotics X Lab</a> from May. 2022 to Oct. 2022, focusing on robot dexterous manipulation.
        </p>
        <div class="social-links">
          <a href="mailto:rayyohhust@gmail.com" target="_blank" rel="noopener noreferrer"><i class="fas fa-envelope"></i> </a> &nbsp/&nbsp
          <a href="static/pdf/YAO_Lei_CV.pdf" target="_blank" rel="noopener noreferrer"><i class="fas fa-file-pdf"></i></a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=pPOcSQMAAAAJ" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> &nbsp/&nbsp
          <a href="https://github.com/RayYoh/" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>&nbsp/&nbsp
          <a href="https://www.linkedin.com/in/rayyoh/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>&nbsp/&nbsp
          <a href="https://twitter.com/HighLander7_" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
        </div>
        <div class="affiliations">
          <img src="static/images/PolyU.png" alt="PolyU Logo" class="affiliation-logo">
          <img src="static/images/penn_logo.png" alt="UPenn Logo" class="affiliation-logo">
          <img src="static/images/tencent_mini.png" alt="Tencent Logo" class="affiliation-logo">
          <img src="static/images/IC_mini.png" alt="IC Logo" class="affiliation-logo">
          <img src="static/images/hust_logo.png" alt="HUST Logo" class="affiliation-logo">
        </div>
      </div>
      <div class="profile-image">
        <img src="static/images/profile_25.png" alt="profile photo" class="hoverZoomLink">
        <p class="image-credit"><em>Photo is generated by Nano Banana</em></p>
      </div>
    </header>

    <main>
      <section id="news">
        <h2>News</h2>
        <ul>
          <li>[Feb. 2026] üéâ <i><a href="https://github.com/RayYoh/LaSSM">LaSSM</a></i> is accepted by <i><strong>IEEE TCSVT</strong></i>. All code and models are released.</li>
          <li>[Jul. 2025] üéâ <i><a href="https://github.com/RayYoh/GaussianCross">GaussianCross</a></i> is accepted by <i><strong>ACM MM 2025</strong></i>. Pretrained weights and downstream code are released.</li>
          <li>[May  2025] üèÜ <i><a href="https://github.com/RayYoh/LaSSM">LaSSM</a></i> and <i><a href="https://rayyoh.github.io/SGIFormer/">SGIFormer</a></i> obtain <strong>first</strong> and <strong>second</strong> places on <a href="https://kaldir.vc.in.tum.de/scannetpp/benchmark/insseg" target="_blank" rel="noopener noreferrer">CVPR 2025 ScanNet++ Challenge</a>, respectively.</li>
          <li>[May  2025] üôè Thanks for Xiaoyang adding <i><a href="https://rayyoh.github.io/SGIFormer/">SGIFormer</a></i> into <a href="https://github.com/Pointcept/Pointcept/commit/5562d1ad6b5dc851d9a45695745304ad092f9186" target="_blank" rel="noopener noreferrer">Pointcept</a>, you can try it in this repo now.</li>
          <li>[Jan. 2025] üéâ Our survey paper about Embodied AI is accepted by <i><strong>Machine Intelligence Research</strong></i>.</li>
          <li>[Nov. 2024] üéâ Our <i>SGIFormer</i> is accepted by <i><strong>IEEE TCSVT</strong></i>. All code and models are released.</li>
          <li>[Oct. 2023] üéâ One paper about robot compliance control is accepted by <i><strong>ROBIO 2023 (Oral)</strong></i>.</li>
          <li>[Aug. 2023] üë®‚Äçüéì I join <i><strong><a href="https://www.eie.polyu.edu.hk/~lpchau/Team.html" target="_blank" rel="noopener noreferrer">JC STEM Lab of ML & CV</a></strong></i> at PolyU as a PhD student.</li>
        </ul>
      </section>

      <section id="research">
        <h2>Research</h2>
        <p>My current research interests lie in <strong>Spatial Intelligence</strong>, with particular interests in 3D scene understanding and 4D world model learning. My research objective is to bulid an embodied agent which has the capability to efficiently perceive and interact with complex real-world environment. Previously, I have worked on robot <strong>compliance control</strong> and <strong>nonprehensile manipulation</strong>.</p>
        <hr>

        <article class="publication">
          <div class="publication-image">
            <img src='static/images/GaussianCross.png' alt="GaussianCross project image">
          </div>
          <div class="publication-details">
            <h3><a href="https://rayyoh.github.io/GaussianCross/">GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting</a></h3>
            <p class="authors"><strong class="me">Lei Yao</strong>, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau</p>
            <p class="links">
              <a href="https://rayyoh.github.io/GaussianCross/">[page]</a> | <a href="https://arxiv.org/abs/2508.02172">[arxiv]</a> | <a href="https://github.com/RayYoh/GaussianCross">[code]</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RayYoh/GaussianCross">
            </p>
            <p class="venue"><em><strong>ACM MM 2025</strong></em></p>
            <p>We present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3DGS techniques.</p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-image">
            <img src='static/images/LaSSM.png' alt="LaSSM project image">
          </div>
          <div class="publication-details">
            <h3><a href="https://github.com/RayYoh/LaSSM">LaSSM: Efficient Semantic-Spatial Query Decoding via Local Aggregation and State Space Models for 3D Instance Segmentation</a></h3>
            <p class="authors"><strong class="me">Lei Yao</strong>, Yi Wang, Yawen Cui, Moyun Liu, Lap-Pui Chau</p>
            <p class="links">
              <a href="https://arxiv.org/abs/2602.11007">[arXiv]</a> | <a href="https://github.com/RayYoh/LaSSM">[code]</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RayYoh/LaSSM">
            </p>
            <p class="venue"><em><strong>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2026</strong></em></p>
            <p class="award">üèÜ <strong>First</strong> place on CVPR 2025 ScanNet++ Challenge.</p>
            <p>We introduce LaSSM, prioritizing simplicity and efficiency while maintaining competitive performance.</p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-image">
            <img src='static/images/SGIFormer.png' alt="SGIFormer project image">
          </div>
          <div class="publication-details">
            <h3><a href="https://rayyoh.github.io/SGIFormer/">SGIFormer: Semantic-guided and Geometric-enhanced Interleaving Transformer for 3D Instance Segmentation</a></h3>
            <p class="authors"><strong class="me">Lei Yao</strong>, Yi Wang, Moyun Liu, Lap-Pui Chau</p>
            <p class="links">
              <a href="https://rayyoh.github.io/SGIFormer/">[page]</a> | <a href="https://ieeexplore.ieee.org/document/10753065">[paper]</a> | <a href="https://arxiv.org/abs/2407.11564">[arxiv]</a> | <a href="https://rayyoh.github.io/SGIFormer/static/visualizations/f3d64c30f8/index.html">[demo]</a> | <a href="https://github.com/RayYoh/SGIFormer">[code]</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RayYoh/SGIFormer">
            </p>
            <p class="venue"><em><strong>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024</strong></em></p>
            <p class="award">üèÜ <strong>Second</strong> place on CVPR 2025 ScanNet++ Challenge*.</p>
            <p class="note">* <em>Reproduced by the organizers based on our code.</em></p>
            <p>We introduce SGIFormer for 3D instance segmentation, which combines semantic-guided query initialization and geometric-enhanced interleaving transformer.</p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-image">
            <img src='static/images/ORCM.png' alt="ORCM project image">
          </div>
          <div class="publication-details">
            <h3><a href="https://arxiv.org/pdf/2408.11537">A Survey of Embodied Learning for Object-Centric Robotic Manipulation</a></h3>
            <p class="authors">Ying Zheng*, <strong class="me">Lei Yao*</strong>, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, Lap-Pui Chau</p>
            <p class="links">
              <a href="https://link.springer.com/article/10.1007/s11633-025-1542-8">[paper]</a> | <a href="https://arxiv.org/pdf/2408.11537">[arxiv]</a> | <a href="https://github.com/RayYoh/OCRM_survey">[code]</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RayYoh/OCRM_survey">
            </p>
            <p class="venue"><em><strong>Machine Intelligence Research, 2025</strong></em></p>
            <p>We provide a comprehensive survey of embodied perceptual learning, embodied policy learning, and embodied task-oriented learning</p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-image">
            <img src='static/images/GANN.png' alt="GANN project image">
          </div>
          <div class="publication-details">
            <h3><a href="https://ieeexplore.ieee.org/abstract/document/10354632" target="_blank" rel="noopener noreferrer">A Data-Driven Phantom Zeros Prediction Algorithm for Traction Force Sensor in Kinesthetic Demonstration</a></h3>
            <p class="authors"><strong class="me">Lei Yao</strong>, Bing Chen, Moyun Liu, Jingming Xie, Youping Chen, Lei He</p>
            <p class="links"><a href="https://ieeexplore.ieee.org/abstract/document/10354632">[paper]</a></p>
            <p class="venue"><em><strong>ROBIO, 2023</strong></em> (<span class="oral-presentation">Oral Presentation</span>)</p>
            <p>We model and predict phantom zeros based on a GA-optimized NN during kinesthetic demonstration.</p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-image">
            <img src='static/images/sensors.png' alt="Sensors project image">
          </div>
          <div class="publication-details">
            <h3><a href="https://www.mdpi.com/1424-8220/21/14/4706" target="_blank" rel="noopener noreferrer">An Integrated Compensation Method for the Force Disturbance of a Six-Axis Force Sensor in Complex Manufacturing Scenarios</a></h3>
            <p class="authors"><strong class="me">Lei Yao</strong>, Qingguang Gao, Dailin Zhang, Wanpeng Zhang, Youping Chen</p>
            <p class="links"><a href="https://www.mdpi.com/1424-8220/21/14/4706">[paper]</a></p>
            <p class="venue"><em><strong>Sensor, 2021</strong></em></p>
            <p>We propose an integrated compensation method to eliminate the disturbances of zero drift, system error, and gravity of robot end-effector.</p>
          </div>
        </article>
      </section>

      <section id="other-publications">
        <h2>Other Publications</h2>
        
        <div class="publication-item">
          <p><a href="">Interaction-aware Representation Modeling With Co-Occurrence Consistency for Egocentric Hand-Object Parsing</a></p>
          <p><em><strong>ICLR, 2026</strong></em><br>Yuejiao Su, Yi Wang, <strong class="me">Lei Yao</strong>, Lap-Pui Chau</p>
        </div>

        <div class="publication-item">
          <p><a href="">MASS: Mesh-inellipse Aligned Deformable Surfel Splatting for Hand Reconstruction and Rendering from Egocentric Monocular Video</a></p>
          <p><em><strong>CVM, 2026</strong></em><br>Haoyu Zhu, Yi Zhang, <strong class="me">Lei Yao</strong>, Lap-Pui Chau, Yi Wang</p>
        </div>

        <div class="publication-item">
          <p><a href="https://arxiv.org/pdf/2512.23176" target="_blank" rel="noopener noreferrer">GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</a></p>
          <p><em><strong>arXiv, 2025</strong></em> | <a href="https://arxiv.org/pdf/2512.23176">[paper]</a><br>Yi Zhang, Yi Wang, <strong class="me">Lei Yao</strong>, Lap-Pui Chau</p>
        </div>

        <div class="publication-item">
          <p><a href="https://www.sciencedirect.com/science/article/pii/S0957417424029610" target="_blank" rel="noopener noreferrer">Dynamic Bottleneck with a Predictable Prior for Image-based Deep Reinforcement Learning</a></p>
          <p><em><strong>Expert Systems with Applications, 2024</strong></em> | <a href="https://github.com/BangYou01/DY2P" target="_blank" rel="noopener noreferrer">[code]</a><br>Bang You, Bing Chen, <strong class="me">Lei Yao</strong>, Youping Chen, Jingming Xie</p>
        </div>

        <div class="publication-item">
          <p><a href="https://ieeexplore.ieee.org/document/10477884" target="_blank" rel="noopener noreferrer">Evaluation of Range Sensing-based Place Recognition for Long-term Urban Localization</a></p>
          <p><em><strong>IEEE Transactions on Intelligent Vehicles (TIV), 2024</strong></em> | <a href="https://github.com/Weixin-Ma/PR_Evaluation_Project" target="_blank" rel="noopener noreferrer">[code]</a><br>Weixin Ma, Huan Yin, <strong class="me">Lei Yao</strong>, Yuxiang Sun, Zhongqing Su</p>
        </div>

        <div class="publication-item">
          <p><a href="https://ieeexplore.ieee.org/abstract/document/10558069" target="_blank" rel="noopener noreferrer">Few-shot Class-agnostic Counting with Occlusion Augmentation and Localization</a></p>
          <p><em><strong>ISCAS, 2024</strong></em><br>Yuejiao Su, Yi Wang, <strong class="me">Lei Yao</strong>, Lap-Pui Chau</p>
        </div>
        
        <details class="previous-pubs">
          <summary><b>Show previous publications</b></summary>
            <div class="publication-item">
              <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253525002520" target="_blank" rel="noopener noreferrer">MAENet: Boost image-guided point cloud completion more accurate and even</a></p>
              <p><em><strong>Information Fusion, 2025</strong></em> | <a href="https://github.com/lmomoy/MAENet" target="_blank" rel="noopener noreferrer">[code]</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lmomoy/MAENet"><br>Moyun Liu, Ziheng Yang, Bing Chen, Youping Chen, Jingming Xie, <strong class="me">Lei Yao</strong>, Lap-Pui Chau, Jiawei Du, Joey Tianyi Zhou</p>
            </div>
            <div class="publication-item">
              <p><a href="https://ieeexplore.ieee.org/document/10517614" target="_blank" rel="noopener noreferrer">Towards Better Unguided Depth Completion via Cross-Modality Knowledge Distillation in the Frequency Domain</a></p>
              <p><em><strong>IEEE Transactions on Intelligent Vehicles (TIV), 2024</strong></em><br>Moyun Liu, Bing Chen, Youping Chen, Jingming Xie, <strong class="me">Lei Yao</strong>, Yang Zhang, Qin Hu, Jiawei Du, Joey Tianyi Zhou</p>
            </div>
            <div class="publication-item">
              <p><a href="https://ieeexplore.ieee.org/abstract/document/10510171/" target="_blank" rel="noopener noreferrer">MENet: Multi-Modal Mapping Enhancement Network for 3D Object Detection in Autonomous Driving</a></p>
              <p><em><strong>IEEE Transactions on Intelligent Transportation Systems, 2024</strong></em><br>Moyun Liu, Youping Chen, Jingming Xie, Yijie Zhu, Yang Zhang, <strong class="me">Lei Yao</strong>, Zhenshan Bing, Genghang Zhuang, Kai Huang, Joey Tianyi Zhou</p>
            </div>
            <div class="publication-item">
              <p><a href="https://www.sciencedirect.com/science/article/pii/S0950705124005112" target="_blank" rel="noopener noreferrer">A Concise but High-performing Network for Image Guided Depth Completion in Autonomous Driving</a></p>
              <p><em><strong>Knowledge-Based Systems, 2024</strong></em> | <a href="https://github.com/lmomoy/CHNet" target="_blank" rel="noopener noreferrer">[code]</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lmomoy/CHNet"><br>Moyun Liu, Bing Chen, Youping Chen, Jingming Xie, <strong class="me">Lei Yao</strong>, Yang Zhang, Joey Tianyi Zhou</p>
            </div>
            <div class="publication-item">
              <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-88977-6_11" target="_blank" rel="noopener noreferrer">SemiT-SAM: Building a Visual Foundation Model for Tooth Instance Segmentation on Panoramic Radiographs</a></p>
              <p><em><strong>MICCAI Workshop, 2024</strong></em> | <a href="https://huggingface.co/datasets/Bryceee/TISI15k-Dataset" target="_blank" rel="noopener noreferrer">[dataset]</a> | <a href="https://github.com/isbrycee/SemiT-SAM" target="_blank" rel="noopener noreferrer">[code]</a><br>Jing Hao, Moyun Liu, Lei He, <strong class="me">Lei Yao</strong>, James Kit Hon Tsoi, Kuo Feng Hung</p>
            </div>
            <div class="publication-item">
              <p><a href="https://www.sciencedirect.com/science/article/pii/S0262885623002093?dgcid=coauthor" target="_blank" rel="noopener noreferrer">BF3D: Bi-directional Fusion 3D Detector With Semantic Sampling and Geometric Mapping</a></p>
              <p><em><strong>Image and Vision Computing, 2023</strong></em> | <a href="https://github.com/hustzyj/BF3D" target="_blank" rel="noopener noreferrer">[code]</a><br>Yijie Zhu, Jingming Xie, Moyun Liu, <strong class="me">Lei Yao</strong>, Youping Chen</p>
            </div>
        </details>
      </section>

      <section id="services">
        <h2>Services</h2>
        <p><strong>Teaching Assistant:</strong></p>
        <ul>
          <li><a href="https://www.polyu.edu.hk/eee/-/media/department/eee/content/study/subject-syllabi/eee-subject-syllabi/msc-subjects/2023-2024/eie546.pdf" target="_blank" rel="noopener noreferrer">EIE546 Video Technology</a> in PolyU, <i>2023Fall, 2024Fall</i></li>
          <li><a href="https://www.eie.polyu.edu.hk/prog/msc/syllabus/eie522.pdf" target="_blank" rel="noopener noreferrer">EIE522 Pattern Recognition: Theory and Applications</a> in PolyU, <i>2024Spring, 2025Spring</i></li>
          <li><a href="https://www.polyu.edu.hk/eee/-/media/department/eee/content/study/subject-syllabi/eee-subject-syllabi/msc-subjects/2023-2024/eie546.pdf" target="_blank" rel="noopener noreferrer">EIE4100 Computer Vision and Pattern Recognition</a> in PolyU, <i>2024Fall</i></li>
        </ul>
        <p><strong>Reviewer:</strong></p>
        <ul>
          <li>ICLR'25, AAAI'26</li>
          <li>TCSVT, TMM</li>
          <li>Pattern Recognition</li>
          <li>Expert Systems with Applications</li>
          <li>The Visual Computer</li>
        </ul>
      </section>

      <section id="education" class="icon-section">
        <h2>Education</h2>
        <div class="entry">
          <div class="icon"><img src="static/images/PolyU.png" alt="PolyU Logo"></div>
          <div class="description">
            <p><strong><a href="https://www.polyu.edu.hk/en/" target="_blank" rel="noopener noreferrer">Hong Kong Polytechnic University <i>(PolyU)</i></a></strong></p>
            <p><em>2023.08 - Present</em></p>
            <p><strong>PhD Student</strong>, <a href="https://www.polyu.edu.hk/eee/" target="_blank" rel="noopener noreferrer">Department of Electrical and Eletronic Engineering</a></p>
            <p>Supervisors: <a href="https://www.eie.polyu.edu.hk/~lpchau/" target="_blank" rel="noopener noreferrer">Prof. Lap-Pui Chau</a></p>
          </div>
        </div>
        <div class="entry">
          <div class="icon"><img src="static/images/hust_logo.png" alt="HUST Logo"></div>
          <div class="description">
            <p><strong><a href="http://english.hust.edu.cn/" target="_blank" rel="noopener noreferrer">Huazhong University of Science and Technology <i>(HUST)</i></a></strong></p>
            <p><em>2020.9 - 2023.6</em></p>
            <p><strong>Master Student</strong>, <a href="http://mse.hust.edu.cn/" target="_blank" rel="noopener noreferrer">School of Mechanical Science and Engineering</a></p>
            <p>Supervisor: <a href="http://english.mse.hust.edu.cn/info/1046/1148.htm" target="_blank" rel="noopener noreferrer">Prof. Youping Chen</a></p>
          </div>
        </div>
        <div class="entry">
          <div class="icon"><img src="static/images/hust_logo.png" alt="HUST Logo"></div>
          <div class="description">
            <p><strong><a href="http://english.hust.edu.cn/" target="_blank" rel="noopener noreferrer">Huazhong University of Science and Technology <i>(HUST)</i></a></strong></p>
            <p><em>2016.9 - 2020.6</em></p>
            <p><strong>Undergraduate Student</strong>, <a href="http://mse.hust.edu.cn/" target="_blank" rel="noopener noreferrer">School of Mechanical Science and Engineering</a></p>
            <p>Supervisor: <a href="http://english.mse.hust.edu.cn/info/1046/1148.htm" target="_blank" rel="noopener noreferrer">Prof. Youping Chen</a></p>
          </div>
        </div>
      </section>

      <section id="experience" class="icon-section">
        <h2>Experience</h2>
        <div class="entry">
          <div class="icon"><img src="static/images/tencent_logo.png" alt="Tencent Logo"></div>
          <div class="description">
            <p><strong><a href="https://www.tencent.com/en-us/business/robotics.html" target="_blank" rel="noopener noreferrer">Tencent Robotics X Lab</a></strong></p>
            <p><em>2022.05 - 2022.10</em></p>
            <p><strong>Research Intern</strong>, Robot Dexterous Manipulation</p>
            <p>Leader: <a href="https://scholar.google.com/citations?hl=en&user=oBXTTesAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener noreferrer">Dr. Yu Zheng</a></p>
          </div>
        </div>
        <div class="entry">
          <div class="icon"><img src="static/images/IC_logo.png" alt="IC Logo"></div>
          <div class="description">
            <p><strong><a href="https://www.imperial.ac.uk/" target="_blank" rel="noopener noreferrer">Imperial College London</a></strong></p>
            <p><em>2019.1 - 2019.2</em></p>
            <p><strong>Winter School</strong>, <a href="https://www.imperial.ac.uk/hamlyn-centre/" target="_blank" rel="noopener noreferrer">Hamlyn Centre</a></p>
          </div>
        </div>
      </section>

      <section id="awards">
        <h2>Selected Awards and Honors</h2>
        <ul>
          <li>2023: <span class="highlight">Hong Kong PolyU Presidental PhD Fellowship <i>(<strong>PPPFS</strong>)</i></span></li>
          <li>2023 & 2020: Outstanding Graduate of <i>HUST</i></li>
          <li>2021: <span class="highlight">National Scholarship</span></li>
          <li>2021: Merit Graduate Student of <i>HUST</i></li>
          <li>2021 & 2020: First-class Graduate School Fellowship</li>
          <li>2019: GoerTek Scholarship (<strong>Sponsored by GoerTek Co., Ltd.</strong>)</li>
          <li>2019: National Encouragement Scholarship</li>
        </ul>
      </section>

      <div id="clustrmap-container">
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=C6pfiXJFA_L1B3P55fU4g85FNhN1Pk-N8fJmWdfX9t4&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353' defer></script>
      </div>
    </main>

    <footer>
      <hr>
      <p>
        This homepage is designed based on <a href="https://jonbarron.info/" target="_blank" rel="noopener noreferrer">Jon Barron</a>'s website and deployed on <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">Github Pages</a>.
        <br> Last updated: Feb. 15 2025
        <br> ¬© Copyright 2026 Lei Yao
      </p>
    </footer>
  </div>
</body>
</html>
